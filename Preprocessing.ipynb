{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>full_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-02 13:25:25</td>\n",
       "      <td>b'@andrerizqonm @IndiHome @TelkomCare @KemenBU...</td>\n",
       "      <td>netral;;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-02 13:23:32</td>\n",
       "      <td>b'Niat mo kerja drmh. Buka laptop. Sambil nont...</td>\n",
       "      <td>negatif;;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-02 13:23:17</td>\n",
       "      <td>b'@IndiHome TOLONG DM DIJAWAB'</td>\n",
       "      <td>netral;;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-02 13:22:41</td>\n",
       "      <td>b'Ini @IndiHome lg knpa ya. Knpa kayak keong \\...</td>\n",
       "      <td>negatif;;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-02 13:20:51</td>\n",
       "      <td>b'@IndiHome Belum ada beb'</td>\n",
       "      <td>netral;;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time                                          full_text  \\\n",
       "0  2021-01-02 13:25:25  b'@andrerizqonm @IndiHome @TelkomCare @KemenBU...   \n",
       "1  2021-01-02 13:23:32  b'Niat mo kerja drmh. Buka laptop. Sambil nont...   \n",
       "2  2021-01-02 13:23:17                     b'@IndiHome TOLONG DM DIJAWAB'   \n",
       "3  2021-01-02 13:22:41  b'Ini @IndiHome lg knpa ya. Knpa kayak keong \\...   \n",
       "4  2021-01-02 13:20:51                         b'@IndiHome Belum ada beb'   \n",
       "\n",
       "       label  \n",
       "0   netral;;  \n",
       "1  negatif;;  \n",
       "2   netral;;  \n",
       "3  negatif;;  \n",
       "4   netral;;  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "df = pd.read_csv(\"@IndiHome.csv\", names=['time','full_text','label'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    b'@andrerizqonm @indihome @telkomcare @kemenbu...\n",
       "1    b'niat mo kerja drmh. buka laptop. sambil nont...\n",
       "2                       b'@indihome tolong dm dijawab'\n",
       "3    b'ini @indihome lg knpa ya. knpa kayak keong \\...\n",
       "4                           b'@indihome belum ada beb'\n",
       "Name: full_text, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lowercase\n",
    "df['full_text'] = df['full_text'].str.lower()\n",
    "df['full_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tweet_special(text):\n",
    "    # remove tab, new line, ans back slice\n",
    "    text = str(text).replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
    "    # remove non ASCII (emoticon, chinese word, .etc)\n",
    "    text = text.encode('ascii', 'replace').decode('ascii')\n",
    "    # remove mention, link, hashtag\n",
    "    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
    "    # remove incomplete URL\n",
    "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "                \n",
    "df['full_text'] = df['full_text'].apply(remove_tweet_special)\n",
    "\n",
    "#remove number\n",
    "def remove_number(text):\n",
    "    return  re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "df['full_text'] = df['full_text'].apply(remove_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Result : \n",
      "\n",
      "0              [segera, berganti, ke, myrepublic, pak]\n",
      "1    [bniat, mo, kerja, drmh, buka, laptop, sambil,...\n",
      "2                                [tolong, dm, dijawab]\n",
      "3    [bini, lg, knpa, ya, knpa, kayak, keong, xfxfx...\n",
      "4                                    [belum, ada, beb]\n",
      "Name: full_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "\n",
    "df['full_text'] = df['full_text'].apply(remove_punctuation)\n",
    "\n",
    "#remove whitespace leading & trailing\n",
    "def remove_whitespace_LT(text):\n",
    "    return text.strip()\n",
    "\n",
    "df['full_text'] = df['full_text'].apply(remove_whitespace_LT)\n",
    "\n",
    "#remove multiple whitespace into single whitespace\n",
    "def remove_whitespace_multiple(text):\n",
    "    return re.sub('\\s+',' ',text)\n",
    "\n",
    "df['full_text'] = df['full_text'].apply(remove_whitespace_multiple)\n",
    "\n",
    "# remove single char\n",
    "def remove_singl_char(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "df['full_text'] = df['full_text'].apply(remove_singl_char)\n",
    "\n",
    "# word tokenize \n",
    "def word_tokenize_wrapper(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "df['full_text'] = df['full_text'].apply(word_tokenize_wrapper)\n",
    "\n",
    "print('Tokenizing Result : \\n') \n",
    "print(df['full_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                               [berganti, myrepublic]\n",
      "1    [bniat, mo, kerja, drmh, buka, laptop, nonton,...\n",
      "2                                         [tolong, dm]\n",
      "3    [bini, lg, knpa, knpa, kayak, keong, xfxfxxadx...\n",
      "4                                                [beb]\n",
      "Name: full_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#stopwords removal\n",
    "\n",
    "# get stopword indonesia\n",
    "list_stopwords = stopwords.words('indonesian')\n",
    "\n",
    "# append additional stopword\n",
    "list_stopwords.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', \n",
    "                       'kalo', 'amp', 'biar', 'bikin', 'bilang', \n",
    "                       'gak', 'ga', 'krn', 'nya', 'nih', 'sih', \n",
    "                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n",
    "                       'jd', 'jgn', 'sdh', 'aja', 'n', 't', \n",
    "                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "                       '&amp', 'yah'])\n",
    "\n",
    "# read txt stopword using pandas\n",
    "txt_stopword = pd.read_csv(\"stopwords.txt\", names= [\"stopwords\"], header = None)\n",
    "\n",
    "# convert stopword string to list & append additional stopword\n",
    "list_stopwords.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
    "\n",
    "# convert list to dictionary\n",
    "list_stopwords = set(list_stopwords)\n",
    "\n",
    "#remove stopword pada list token\n",
    "def stopwords_removal(words):\n",
    "    return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "df['full_text'] = df['full_text'].apply(stopwords_removal)\n",
    "\n",
    "print(df['full_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                               [berganti, myrepublic]\n",
      "1    [bniat, mo, kerja, drmh, buka, laptop, nonton,...\n",
      "2                                         [tolong, dm]\n",
      "3    [bini, lg, knpa, knpa, kayak, keong, xfxfxxadx...\n",
      "4                                                [beb]\n",
      "Name: full_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#lemmatization\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "df['full_text'] = df['full_text'].apply(lemmatize_verbs)\n",
    "\n",
    "print(df['full_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv\n",
    "df.to_csv(\"dataset2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into 80% training (X_train & y_train) and 20% testing (X_test & y_test) data sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, df['label'], test_size = 0.20, random_state = 0)\n",
    "\n",
    "# Get the shape of messages_bow\n",
    "# messages_bow.shape\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "X_train_tf = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "NBNaiveBayes = MultinomialNB()\n",
    "NBNaiveBayes.fit(X_train_tfidf, y_train)\n",
    "print(NBNaiveBayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tf = count_vect.transform(X_test)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_tf)\n",
    "\n",
    "predicted = NBNaiveBayes.predict(X_test_tfidf)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predicted, target_names=['negatif','netral','positif']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
